{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from rl.dynamic_programming import V, S, A\n",
    "from rl import dynamic_programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import markov_process, markov_decision_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Mapping, Iterator, TypeVar, Tuple, Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Categorical, Choose\n",
    "from rl.iterate import converged, iterate\n",
    "from rl.markov_process import NonTerminal, State\n",
    "from rl.markov_decision_process import (FiniteMarkovDecisionProcess,\n",
    "                                        FiniteMarkovRewardProcess)\n",
    "from rl.policy import FinitePolicy, FiniteDeterministicPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.midterm_2022.priority_q import  PriorityQueue\n",
    "from rl.midterm_2022 import grid_maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will implement the gaps-based value iteration algorithm mentioned in class.\n",
    "\n",
    "The gaps-based iteration algorithm proceeds as follows\n",
    "\n",
    "1. Initialize the value function to zero for all states: $v[s] = 0\\ \\forall s \\in \\mathcal{N}$\n",
    "2. Calculate the gaps for each state: $g[s] = |v[s] - \\max_a \\mathcal{R}(s,a) + \\sum_{s'} \\mathcal{P}(s,a,s') \\cdot v(s')|$\n",
    "3. While there is some gap that exceeds a threshold\n",
    " - Select the state with the  largest gap: $s_{max} = \\arg\\max_{s \\in \\mathcal{N}} g[s]$\n",
    " - Update the value function for $s_{max}$: $v[s_{max}] = \\max_a \\mathcal{R}(s_{max},a) + \\sum_{s'}\\mathcal{P}(s_{max},a,s') \\cdot v(s')$\n",
    " -  Update the gap for $s_{max}$: $g[s_{max}] = 0$\n",
    "4. Return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test your implementation on a grid maze MDP. We have defined this class in \"grid_maze.py\", you should  briefly familiarize yourself with that code. In particular pay attention to the difference in reward functions for the two classes \"GridMazeMDP_Dense\" and \"GridMazeMDP_Sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can use the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(2, 3)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can visualize the maze if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "|*  |\n",
      "| +-+\n",
      "|   |\n",
      "| + +\n",
      "| | |\n",
      "|-+-+\n"
     ]
    }
   ],
   "source": [
    "print(maze_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also visualize a policy on the mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "print(maze_mdp.print_policy(v2_res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to make use of the PriorityQueue class in your implementation. A PriorityQueue is an ordered queue which supports the following operations\n",
    "1. isEmpty(self): check if the queue is empty   \n",
    "2. contains(self, element): check if the queue contains an element\n",
    "3. peek(self): peek at the highest priority element in the queue    \n",
    "4. pop(self): remove and return the highest priority element in the queue    \n",
    "5. insert(self, element, priority): insert an element into the queue with given priority\n",
    "6. update(self, element, new_priority): update the priority of an element in the queue\n",
    "7. delete(self, element): delete an element from the queue\n",
    "\n",
    "Below are some examples of using the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.3, 'b') (0.4, 'a') (0.56, 'c')\n",
      "(-0.4, 'b') (0.56, 'c') (-0.3, 'a')\n"
     ]
    }
   ],
   "source": [
    "q: PriorityQueue = PriorityQueue()\n",
    "# print(q.isEmpty(), ':', \"the queue is empty\")\n",
    "q.insert(\"a\", 0.4)\n",
    "# print(q.isEmpty(), ':',  \"the queue is not empty\")\n",
    "# print(q.contains(\"a\"), ':',  \"the queue contains a\")\n",
    "# print(q.contains(\"b\"), ':',  \"the queue does not contain a\")\n",
    "# print(q.peek(), ':',  \"a is the first element in the queue\")\n",
    "q.insert(\"b\", -0.3)\n",
    "# print(q.contains(\"b\"), ':',  \"the queue now contains b\")\n",
    "# print(q.peek(), ':',  \"b is now at the front of the queue\")\n",
    "# x = q.pop()\n",
    "# print(x, ':',  \"we removed b from the queue\")\n",
    "# print(q.isEmpty(), ':',  \"the queue still nonempty\")\n",
    "# print(q.contains(\"a\"), ':',  \"the queue still contains a\")\n",
    "# print(q.contains(\"b\"), ':',  \"the queue does not contain b anymore\")\n",
    "# print(q.peek(), ':',  \"a is at the front of the queue\")\n",
    "q.insert(\"c\", 0.56)\n",
    "# print(q.peek(), ':',  \"a is still at the front of the queue\")\n",
    "print(q)\n",
    "q.update(\"a\", -0.3)\n",
    "q.update(\"b\", -0.4)\n",
    "print(q)\n",
    "# print(q.peek(), ':',  \"after updating a is no longer at the front of the queue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def invert_transition_map(mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A]) -> Mapping[S, Iterable[S]]:\n",
    "        inverted_mapping = {}\n",
    "\n",
    "        for curr_state, act_dict in mdp.mapping.items():\n",
    "                for _, sr_dict in act_dict.items():\n",
    "                        for (next_state, _), _ in sr_dict.table().items():\n",
    "                                if next_state not in inverted_mapping:\n",
    "                                        inverted_mapping[next_state] = [curr_state]\n",
    "                                else:\n",
    "                                        inverted_mapping[next_state].append(curr_state)\n",
    "\n",
    "        return inverted_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TOLERANCE = 1e-6\n",
    "\n",
    "\n",
    "def gaps_value_iteration(\n",
    "    mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float, \n",
    "    gaps: PriorityQueue) -> Iterator[V[S]]:\n",
    "    '''\n",
    "    Calculate the value function (V*) of the given MDP by applying the\n",
    "    update function repeatedly until the values converge.\n",
    "\n",
    "    '''\n",
    "    dependency_map = invert_transition_map(mdp)\n",
    "    v_0: V[S] = {s: 0.0 for s in mdp.non_terminal_states}\n",
    "    \n",
    "        \n",
    "    def update(v: V[S]) -> V[S]:\n",
    "        '''\n",
    "        YOUR CODE HERE\n",
    "        perform a single update to v for the state with the largest gap\n",
    "        update the gaps for any dependent states\n",
    "        '''\n",
    "        s_max : S = gaps.pop()\n",
    "        v[s_max] = max(mdp.mapping[s_max][a].expectation(lambda sr : sr[1] + gamma * dynamic_programming.extended_vf(v, sr[0])) for a in mdp.actions(s_max))\n",
    "\n",
    "        for s in dependency_map:\n",
    "            if gaps.contains(s) and isinstance(s, NonTerminal):\n",
    "                max_val = max(mdp.mapping[s][a].expectation(lambda sr : sr[1] + gamma * dynamic_programming.extended_vf(v, sr[0])) for a in mdp.actions(s))\n",
    "                gaps.update(s, - abs(v[s] - max_val))\n",
    "\n",
    "        return v\n",
    "\n",
    "    \n",
    "    return iterate(update, v_0)\n",
    "\n",
    "\n",
    "def gaps_value_iteration_result(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "    \n",
    "    gaps = PriorityQueue()\n",
    "    \n",
    "    for s in mdp.non_terminal_states:\n",
    "        max_val = max(mdp.mapping[s][a].expectation(lambda sr : sr[1]) for a in mdp.actions(s)) # vf for next state is 0\n",
    "        gaps.insert(s, - abs(max_val))\n",
    "\n",
    "    \n",
    "    def criterion(v1 : V[S], v2 : V[S], tolerance : float = DEFAULT_TOLERANCE) -> bool:\n",
    "        return max(abs(v1[s] - v2[s]) for s in v1) < tolerance\n",
    "        \n",
    "    opt_vf: V[S] = converged(\n",
    "        gaps_value_iteration(mdp, gamma, gaps),\n",
    "        done=criterion \n",
    "    )\n",
    "        \n",
    "    opt_policy: markov_decision_process.FiniteDeterministicPolicy[S, A] = dynamic_programming.greedy_policy_from_vf(\n",
    "        mdp,\n",
    "        opt_vf,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    return opt_vf, opt_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not change the code below here, just run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with sparse rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(5, 5)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For State GridState(x=0, y=1): Do Action N\n",
      "For State GridState(x=0, y=2): Do Action N\n",
      "For State GridState(x=0, y=3): Do Action N\n",
      "For State GridState(x=0, y=4): Do Action E\n",
      "For State GridState(x=1, y=0): Do Action W\n",
      "For State GridState(x=1, y=1): Do Action S\n",
      "For State GridState(x=1, y=2): Do Action E\n",
      "For State GridState(x=1, y=3): Do Action S\n",
      "For State GridState(x=1, y=4): Do Action W\n",
      "For State GridState(x=2, y=0): Do Action W\n",
      "For State GridState(x=2, y=1): Do Action E\n",
      "For State GridState(x=2, y=2): Do Action W\n",
      "For State GridState(x=2, y=3): Do Action S\n",
      "For State GridState(x=2, y=4): Do Action W\n",
      "For State GridState(x=3, y=0): Do Action E\n",
      "For State GridState(x=3, y=1): Do Action W\n",
      "For State GridState(x=3, y=2): Do Action W\n",
      "For State GridState(x=3, y=3): Do Action N\n",
      "For State GridState(x=3, y=4): Do Action W\n",
      "For State GridState(x=4, y=0): Do Action W\n",
      "For State GridState(x=4, y=1): Do Action W\n",
      "For State GridState(x=4, y=2): Do Action S\n",
      "For State GridState(x=4, y=3): Do Action N\n",
      "For State GridState(x=4, y=4): Do Action W\n",
      "\n",
      "0.0024449825286865234\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 0.9)\n",
    "print(v1_res[1])\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For State GridState(x=0, y=1): Do Action N\n",
      "For State GridState(x=0, y=2): Do Action N\n",
      "For State GridState(x=0, y=3): Do Action N\n",
      "For State GridState(x=0, y=4): Do Action E\n",
      "For State GridState(x=1, y=0): Do Action W\n",
      "For State GridState(x=1, y=1): Do Action N\n",
      "For State GridState(x=1, y=2): Do Action N\n",
      "For State GridState(x=1, y=3): Do Action N\n",
      "For State GridState(x=1, y=4): Do Action N\n",
      "For State GridState(x=2, y=0): Do Action W\n",
      "For State GridState(x=2, y=1): Do Action N\n",
      "For State GridState(x=2, y=2): Do Action W\n",
      "For State GridState(x=2, y=3): Do Action S\n",
      "For State GridState(x=2, y=4): Do Action W\n",
      "For State GridState(x=3, y=0): Do Action E\n",
      "For State GridState(x=3, y=1): Do Action W\n",
      "For State GridState(x=3, y=2): Do Action W\n",
      "For State GridState(x=3, y=3): Do Action N\n",
      "For State GridState(x=3, y=4): Do Action W\n",
      "For State GridState(x=4, y=0): Do Action S\n",
      "For State GridState(x=4, y=1): Do Action W\n",
      "For State GridState(x=4, y=2): Do Action N\n",
      "For State GridState(x=4, y=3): Do Action N\n",
      "For State GridState(x=4, y=4): Do Action W\n",
      "\n",
      "0.0016031265258789062\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "\n",
    "print(v2_res[1])\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/30/4m8g7n6n6692pdsg7v_fqlxc0000gn/T/ipykernel_78875/4042142549.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mv1_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mv2_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with dense rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_mdp = grid_maze.GridMazeMDP_Dense(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For State GridState(x=0, y=1): Do Action S\n",
      "For State GridState(x=0, y=2): Do Action E\n",
      "For State GridState(x=0, y=3): Do Action S\n",
      "For State GridState(x=0, y=4): Do Action N\n",
      "For State GridState(x=1, y=0): Do Action W\n",
      "For State GridState(x=1, y=1): Do Action E\n",
      "For State GridState(x=1, y=2): Do Action W\n",
      "For State GridState(x=1, y=3): Do Action E\n",
      "For State GridState(x=1, y=4): Do Action N\n",
      "For State GridState(x=2, y=0): Do Action W\n",
      "For State GridState(x=2, y=1): Do Action W\n",
      "For State GridState(x=2, y=2): Do Action W\n",
      "For State GridState(x=2, y=3): Do Action W\n",
      "For State GridState(x=2, y=4): Do Action E\n",
      "For State GridState(x=3, y=0): Do Action W\n",
      "For State GridState(x=3, y=1): Do Action N\n",
      "For State GridState(x=3, y=2): Do Action W\n",
      "For State GridState(x=3, y=3): Do Action W\n",
      "For State GridState(x=3, y=4): Do Action W\n",
      "For State GridState(x=4, y=0): Do Action W\n",
      "For State GridState(x=4, y=1): Do Action S\n",
      "For State GridState(x=4, y=2): Do Action N\n",
      "For State GridState(x=4, y=3): Do Action S\n",
      "For State GridState(x=4, y=4): Do Action W\n",
      "\n",
      "0.0034890174865722656\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 1)\n",
    "print(v1_res[1])\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For State GridState(x=0, y=1): Do Action N\n",
      "For State GridState(x=0, y=2): Do Action N\n",
      "For State GridState(x=0, y=3): Do Action N\n",
      "For State GridState(x=0, y=4): Do Action N\n",
      "For State GridState(x=1, y=0): Do Action W\n",
      "For State GridState(x=1, y=1): Do Action E\n",
      "For State GridState(x=1, y=2): Do Action W\n",
      "For State GridState(x=1, y=3): Do Action E\n",
      "For State GridState(x=1, y=4): Do Action N\n",
      "For State GridState(x=2, y=0): Do Action W\n",
      "For State GridState(x=2, y=1): Do Action S\n",
      "For State GridState(x=2, y=2): Do Action W\n",
      "For State GridState(x=2, y=3): Do Action N\n",
      "For State GridState(x=2, y=4): Do Action E\n",
      "For State GridState(x=3, y=0): Do Action W\n",
      "For State GridState(x=3, y=1): Do Action N\n",
      "For State GridState(x=3, y=2): Do Action W\n",
      "For State GridState(x=3, y=3): Do Action W\n",
      "For State GridState(x=3, y=4): Do Action N\n",
      "For State GridState(x=4, y=0): Do Action W\n",
      "For State GridState(x=4, y=1): Do Action N\n",
      "For State GridState(x=4, y=2): Do Action N\n",
      "For State GridState(x=4, y=3): Do Action S\n",
      "For State GridState(x=4, y=4): Do Action W\n",
      "\n",
      "0.003683805465698242\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 1)\n",
    "print(v2_res[1])\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/30/4m8g7n6n6692pdsg7v_fqlxc0000gn/T/ipykernel_78875/4042142549.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mv1_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mv2_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
