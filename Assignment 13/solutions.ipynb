{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import numpy as np \n",
    "from pprint import pprint\n",
    "from typing import TypeVar, Iterable, Mapping, Dict, Callable, Iterator\n",
    "import matplotlib.pyplot as plt \n",
    "import rl.markov_process as mp\n",
    "import itertools\n",
    "import rl.markov_decision_process as mdp\n",
    "import rl.td as td \n",
    "from rl.distribution import Choose, Categorical\n",
    "from rl.function_approx import LinearFunctionApprox, Tabular\n",
    "import rl.chapter11.control_utils as control\n",
    "import rl.iterate as iterate\n",
    "import rl.policy as policy\n",
    "from rl.approximate_dynamic_programming import QValueFunctionApprox, NTStateDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing SARSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = TypeVar(\"S\")\n",
    "# A = TypeVar(\"A\")\n",
    "\n",
    "\n",
    "# def sarsa_control(mdp : mdp.MarkovDecisionProcess[S, A],\n",
    "#                   start_state_distribution : NTStateDistribution[S],\n",
    "#                   approx_0 : QValueFunctionApprox[S, A],\n",
    "#                   gamma : float,\n",
    "#                   eps_decay_func : Callable[[int], float],\n",
    "#                   max_episode_length : int) -> Iterator[QValueFunctionApprox[S, A]]:\n",
    "#     q : QValueFunctionApprox[S, A] = approx_0\n",
    "#     num_episodes : int = 0\n",
    "#     yield q\n",
    "\n",
    "#     while True:\n",
    "#         num_episodes += 1\n",
    "#         eps : float = eps_decay_func(num_episodes)\n",
    "#         state : S = start_state_distribution.sample()\n",
    "#         action : A = td.epsilon_greedy_action(q=q, \n",
    "#                                               nt_state=state, \n",
    "#                                               actions=set(mdp.actions(state)), \n",
    "#                                               ε=eps)\n",
    "        \n",
    "#         steps : int = 0\n",
    "\n",
    "#         while isinstance(state, mp.NonTerminal) and steps < max_episode_length:\n",
    "#             next_state, reward = mdp.step(state, action).sample()\n",
    "#             if isinstance(next_state, mp.NonTerminal):\n",
    "#                 next_act : A = td.epsilon_greedy_action(q=q, \n",
    "#                                                         nt_state=next_state, \n",
    "#                                                         actions=set(mdp.actions(next_state)), \n",
    "#                                                         ε=steps)\n",
    "#                 q.update([((state, action), reward + gamma * q((next_state, next_act)))])\n",
    "#                 action = next_act\n",
    "#             else:\n",
    "#                 q.update([((state, action), reward)])\n",
    "            \n",
    "#             yield q\n",
    "#             state = next_state\n",
    "#             steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Policy Iteration Optimal Value Function and Optimal Policy\n",
      "----------------------------\n",
      "{NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.661,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.992,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.661,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.895,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.992,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.992}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.chapter3.simple_inventory_mdp_cap import *\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "    SimpleInventoryMDPCap(\n",
    "        capacity=user_capacity,\n",
    "        poisson_lambda=user_poisson_lambda,\n",
    "        holding_cost=user_holding_cost,\n",
    "        stockout_cost=user_stockout_cost\n",
    "    )\n",
    "\n",
    "print(\"MDP Policy Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\" * 2)\n",
    "\n",
    "opt_vf_pi, opt_policy_pi = policy_iteration_result(si_mdp, gamma=user_gamma)\n",
    "\n",
    "pprint({k : round(v, 3) for k, v in opt_vf_pi.items()})\n",
    "print()\n",
    "print(opt_policy_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Tabular approximation for the Q-value function and implementing SARSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -20.844,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -15.158,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -14.807,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -16.004,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -16.176,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -16.842}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "num_steps = 1000\n",
    "# ffs = [(lambda x, s=s, a=a : 1.0 if x[0] == s and x[1] == a else 0.0) for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)]\n",
    "# approx_0 = LinearFunctionApprox.create(feature_functions=ffs, regularization_coeff=0.01)\n",
    "approx_0 = Tabular()\n",
    "\n",
    "# sarsa_qvf = sarsa_control(mdp=si_mdp, start_state_distribution=Choose(si_mdp.non_terminal_states), approx_0=approx_0,\n",
    "#                           gamma=user_gamma, eps_decay_func=lambda k : 1. / k, max_episode_length=num_steps)\n",
    "sarsa_qvf = td.glie_sarsa(mdp=si_mdp, states=Choose(si_mdp.non_terminal_states), approx_0=approx_0,\n",
    "                          γ=user_gamma, ϵ_as_func_of_episodes=lambda k : 1. / k, max_episode_length=num_steps)\n",
    "\n",
    "*_, opt_qvf = itertools.islice(sarsa_qvf, num_episodes)\n",
    "opt_vf, opt_policy = control.get_vf_and_policy_from_qvf(mdp=si_mdp, qvf=opt_qvf)\n",
    "\n",
    "pprint({s : round(v, 3) for s, v in opt_vf.items()})\n",
    "print()\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/30/4m8g7n6n6692pdsg7v_fqlxc0000gn/T/ipykernel_46254/2417154795.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mqlearning_qvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msi_mdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_from_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGetBehaviorPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mChoose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msi_mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_terminal_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapprox_0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapprox_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mγ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_qvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqlearning_qvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mopt_vf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vf_and_policy_from_qvf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msi_mdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqvf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_qvf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Classes/CME 241/CME-241-RLinFinance/Assignment 13/../rl/td.py\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(mdp, policy_from_q, states, approx_0, γ, max_episode_length)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNonTerminal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_episode_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_from_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             next_return: float = max(\n",
      "\u001b[0;32m~/Desktop/Classes/CME 241/CME-241-RLinFinance/Assignment 13/../rl/policy.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNonTerminal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mChoose\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mChoose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/30/4m8g7n6n6692pdsg7v_fqlxc0000gn/T/ipykernel_46254/2417154795.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(state, mdp_)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarkovDecisionProcess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNonTerminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmdp_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_terminal_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmdp_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNonTerminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#     return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "S = TypeVar(\"S\")\n",
    "A = TypeVar(\"A\")\n",
    "\n",
    "def GetBehaviorPolicy(q : QValueFunctionApprox[S, A], mdp_ : mdp.MarkovDecisionProcess[S, A]) -> policy.Policy[S, A]:\n",
    "    def f(state : S, mdp_ : mdp.MarkovDecisionProcess[S, A] = mdp_) -> Iterable[A]:\n",
    "        if mp.NonTerminal(state) in mdp_.non_terminal_states:\n",
    "            return mdp_.actions[mp.NonTerminal(state)]\n",
    "    return policy.UniformPolicy(valid_actions=f)\n",
    "\n",
    "\n",
    "\n",
    "num_episodes = 1000\n",
    "num_steps = 1000\n",
    "# ffs = [(lambda x, s=s, a=a : 1.0 if x[0] == s and x[1] == a else 0.0) for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)]\n",
    "# approx_0 = LinearFunctionApprox.create(feature_functions=ffs, regularization_coeff=0.01)\n",
    "approx_0 = Tabular()\n",
    "\n",
    "qlearning_qvf = td.q_learning(mdp=si_mdp, policy_from_q=lambda q, m : mc.epsilon_greedy_policy(q=q, mdp=m, ε), \n",
    "                              states=Choose(si_mdp.non_terminal_states), approx_0=approx_0, γ=user_gamma, max_episode_length=num_steps)\n",
    "\n",
    "*_, opt_qvf = itertools.islice(qlearning_qvf, num_episodes)\n",
    "opt_vf, opt_policy = control.get_vf_and_policy_from_qvf(mdp=si_mdp, qvf=opt_qvf)\n",
    "\n",
    "pprint({s : round(v, 3) for s, v in opt_vf.items()})\n",
    "print()\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
