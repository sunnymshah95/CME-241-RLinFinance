{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import numpy as np \n",
    "from pprint import pprint\n",
    "from typing import TypeVar, Iterable, Mapping, Dict, Callable, Iterator\n",
    "import matplotlib.pyplot as plt \n",
    "import rl.markov_process as mp\n",
    "import itertools\n",
    "import rl.markov_decision_process as mdp\n",
    "import rl.monte_carlo as mc\n",
    "import rl.td as td \n",
    "from rl.distribution import Choose, Categorical\n",
    "from rl.function_approx import LinearFunctionApprox, Tabular\n",
    "import rl.chapter11.control_utils as control\n",
    "import rl.iterate as iterate\n",
    "import rl.policy as policy\n",
    "from rl.approximate_dynamic_programming import QValueFunctionApprox, NTStateDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing SARSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = TypeVar(\"S\")\n",
    "# A = TypeVar(\"A\")\n",
    "\n",
    "\n",
    "# def sarsa_control(mdp : mdp.MarkovDecisionProcess[S, A],\n",
    "#                   start_state_distribution : NTStateDistribution[S],\n",
    "#                   approx_0 : QValueFunctionApprox[S, A],\n",
    "#                   gamma : float,\n",
    "#                   eps_decay_func : Callable[[int], float],\n",
    "#                   max_episode_length : int) -> Iterator[QValueFunctionApprox[S, A]]:\n",
    "#     q : QValueFunctionApprox[S, A] = approx_0\n",
    "#     num_episodes : int = 0\n",
    "#     yield q\n",
    "\n",
    "#     while True:\n",
    "#         num_episodes += 1\n",
    "#         eps : float = eps_decay_func(num_episodes)\n",
    "#         state : S = start_state_distribution.sample()\n",
    "#         action : A = td.epsilon_greedy_action(q=q, \n",
    "#                                               nt_state=state, \n",
    "#                                               actions=set(mdp.actions(state)), \n",
    "#                                               ε=eps)\n",
    "        \n",
    "#         steps : int = 0\n",
    "\n",
    "#         while isinstance(state, mp.NonTerminal) and steps < max_episode_length:\n",
    "#             next_state, reward = mdp.step(state, action).sample()\n",
    "#             if isinstance(next_state, mp.NonTerminal):\n",
    "#                 next_act : A = td.epsilon_greedy_action(q=q, \n",
    "#                                                         nt_state=next_state, \n",
    "#                                                         actions=set(mdp.actions(next_state)), \n",
    "#                                                         ε=steps)\n",
    "#                 q.update([((state, action), reward + gamma * q((next_state, next_act)))])\n",
    "#                 action = next_act\n",
    "#             else:\n",
    "#                 q.update([((state, action), reward)])\n",
    "            \n",
    "#             yield q\n",
    "#             state = next_state\n",
    "#             steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Policy Iteration Optimal Value Function and Optimal Policy\n",
      "----------------------------\n",
      "{NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.992,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.992,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.661,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.992,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.661,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.895}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.chapter3.simple_inventory_mdp_cap import *\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "    SimpleInventoryMDPCap(\n",
    "        capacity=user_capacity,\n",
    "        poisson_lambda=user_poisson_lambda,\n",
    "        holding_cost=user_holding_cost,\n",
    "        stockout_cost=user_stockout_cost\n",
    "    )\n",
    "\n",
    "print(\"MDP Policy Iteration Optimal Value Function and Optimal Policy\")\n",
    "print(\"--------------\" * 2)\n",
    "\n",
    "opt_vf_pi, opt_policy_pi = policy_iteration_result(si_mdp, gamma=user_gamma)\n",
    "\n",
    "pprint({k : round(v, 3) for k, v in opt_vf_pi.items()})\n",
    "print()\n",
    "print(opt_policy_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Tabular approximation for the Q-value function and implementing SARSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -21.543,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -14.053,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -15.139,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -14.666,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -15.537,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -17.051}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "num_steps = 1000\n",
    "# ffs = [(lambda x, s=s, a=a : 1.0 if x[0] == s and x[1] == a else 0.0) for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)]\n",
    "# approx_0 = LinearFunctionApprox.create(feature_functions=ffs, regularization_coeff=0.01)\n",
    "approx_0 = Tabular()\n",
    "\n",
    "# sarsa_qvf = sarsa_control(mdp=si_mdp, start_state_distribution=Choose(si_mdp.non_terminal_states), approx_0=approx_0,\n",
    "#                           gamma=user_gamma, eps_decay_func=lambda k : 1. / k, max_episode_length=num_steps)\n",
    "sarsa_qvf = td.glie_sarsa(mdp=si_mdp, states=Choose(si_mdp.non_terminal_states), approx_0=approx_0,\n",
    "                          γ=user_gamma, ϵ_as_func_of_episodes=lambda k : 1. / k, max_episode_length=num_steps)\n",
    "\n",
    "*_, opt_qvf = itertools.islice(sarsa_qvf, num_episodes)\n",
    "opt_vf, opt_policy = control.get_vf_and_policy_from_qvf(mdp=si_mdp, qvf=opt_qvf)\n",
    "\n",
    "pprint({s : round(v, 3) for s, v in opt_vf.items()})\n",
    "print()\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -18.574,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -12.228,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -11.603,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -12.853,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -13.161,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -14.639}\n",
      "\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "num_steps = 1000\n",
    "# ffs = [(lambda x, s=s, a=a : 1.0 if x[0] == s and x[1] == a else 0.0) for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)]\n",
    "# approx_0 = LinearFunctionApprox.create(feature_functions=ffs, regularization_coeff=0.01)\n",
    "approx_0 = Tabular()\n",
    "\n",
    "qlearning_qvf = td.q_learning(mdp=si_mdp, policy_from_q=lambda q, m : mc.epsilon_greedy_policy(q=q, mdp=m, ε=0.01), \n",
    "                              states=Choose(si_mdp.non_terminal_states), approx_0=approx_0, γ=user_gamma, max_episode_length=num_steps)\n",
    "\n",
    "*_, opt_qvf = itertools.islice(qlearning_qvf, num_episodes)\n",
    "opt_vf, opt_policy = control.get_vf_and_policy_from_qvf(mdp=si_mdp, qvf=opt_qvf)\n",
    "\n",
    "pprint({s : round(v, 3) for s, v in opt_vf.items()})\n",
    "print()\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
