{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Mapping, Dict, Tuple\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.distribution import Categorical, FiniteDistribution, Distribution\n",
    "from rl.dynamic_programming import value_iteration, value_iteration_result\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: f\"{x:.3f}\"})\n",
    "plt.rcParams['figure.figsize'] = (15, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE = 'SPACE'\n",
    "GOAL = 'GOAL'\n",
    "BLOCK = 'BLOCK'\n",
    "\n",
    "DOWN = 'D'\n",
    "UP = 'U'\n",
    "LEFT = 'L'\n",
    "RIGHT = 'R'\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Position:\n",
    "    x : int\n",
    "    y : int\n",
    "\n",
    "ActionMapping = Mapping[Position, Mapping[str, Categorical[Tuple[Position, float]]]]\n",
    "\n",
    "class GridMaze1(FiniteMarkovDecisionProcess[Position, str]):\n",
    "    '''\n",
    "    here, the rewards formulation is that every step gets a reward of -1 \n",
    "    the terminal state has a reward of 0\n",
    "    '''\n",
    "    def __init__(self, grid : Dict[Tuple[int, int], str]) -> None:\n",
    "        self.grid = grid\n",
    "        super().__init__(self.GetActionMapping())\n",
    "\n",
    "    def GetActionMapping(self) -> ActionMapping:\n",
    "        d : Dict[Position, Dict[str, Categorical[Tuple[Position, float]]]] = {}\n",
    "\n",
    "        for pos, kind in self.grid.items():\n",
    "            sub_d : Dict[str, Categorical[Tuple[Position, float]]] = {}\n",
    "            if kind == SPACE:\n",
    "                state = Position(x = pos[0], y = pos[1])\n",
    "\n",
    "                down = self.grid.get((pos[0] + 1, pos[1]), BLOCK)\n",
    "                up = self.grid.get((pos[0] - 1, pos[1]), BLOCK)\n",
    "                right = self.grid.get((pos[0], pos[1] + 1), BLOCK)\n",
    "                left = self.grid.get((pos[0], pos[1] - 1), BLOCK)\n",
    "\n",
    "                num_options = (left == SPACE) + (right == SPACE) + (up == SPACE) + (down == SPACE) + (left == GOAL) + (right == GOAL) + (up == GOAL) + (down == GOAL)\n",
    "                p = float(1 / num_options)\n",
    "\n",
    "                if down == SPACE:\n",
    "                    sub_d[DOWN] = Categorical({(Position(x=pos[0] + 1, y=pos[1]), -1.0) : p})\n",
    "\n",
    "                elif down == GOAL:\n",
    "                    sub_d[DOWN] = Categorical({(Position(x=pos[0] + 1, y=pos[1]), 0.0) : p})\n",
    "                \n",
    "                if up == SPACE:\n",
    "                    sub_d[UP] = Categorical({(Position(x=pos[0] - 1, y=pos[1]), -1.0): p})\n",
    "\n",
    "                elif up == GOAL:\n",
    "                    sub_d[UP] = Categorical({(Position(x=pos[0] - 1, y=pos[1]), 0.0): p})\n",
    "                \n",
    "                if right == SPACE:\n",
    "                    sub_d[RIGHT] = Categorical({(Position(x=pos[0], y=pos[1] + 1), -1.0): p})\n",
    "\n",
    "                elif right == GOAL:\n",
    "                    sub_d[RIGHT] = Categorical({(Position(x=pos[0], y=pos[1] + 1), 0.0): p})\n",
    "                \n",
    "                if left == SPACE:\n",
    "                    sub_d[LEFT] = Categorical({(Position(x=pos[0], y=pos[1] - 1), -1.0): p})\n",
    "                elif left == GOAL:\n",
    "                    sub_d[LEFT] = Categorical({(Position(x=pos[0], y=pos[1] - 1), 0.0): p})\n",
    "\n",
    "\n",
    "                d[state] = sub_d\n",
    "\n",
    "        return d\n",
    "\n",
    "def done(v1 : Dict[Position, float], v2 : Dict[Position, float], tol : float):\n",
    "\t'''\n",
    "\tthis function takes in two dictionaries, converts them to numpy\n",
    "\tarrays and then returns True if the maximum absolute value across\n",
    "\tone array and the other is smaller than the specified tolerance\n",
    "\n",
    "\tparameters:\n",
    "\tv1: a dictionary with states as the keys and the elements\n",
    "\t\tin the value function as the values\n",
    "\tv2: a dictionary with states as the keys and the elements\n",
    "\t\tin the value function as the values\n",
    "\ttol: the specified tolerance\n",
    "\n",
    "\treturns:\n",
    "\t--------\n",
    "\tTrue if the maximum difference is less than TOLERANCE\n",
    "\tFalse otherwise\n",
    "\t'''\n",
    "\tarray1 = np.array([i for i in v1.values()])\n",
    "\tarray2 = np.array([i for i in v2.values()])\n",
    "\n",
    "\treturn np.linalg.norm(array1 - array2, ord = np.inf) < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0' '-' '0' '0' '0' '0' '0' '0']\n",
      " ['0' '-' '-' '0' '-' '-' '-' '-']\n",
      " ['0' '-' '0' '0' '0' '0' '-' '0']\n",
      " ['0' '0' '0' '-' '-' '0' '-' '0']\n",
      " ['0' '-' '0' '-' '0' '0' '0' '0']\n",
      " ['-' '-' '0' '-' '0' '-' '0' '-']\n",
      " ['0' '-' '-' '-' '0' '-' '0' '0']\n",
      " ['0' '0' '0' '0' '0' '-' '-' 'x']]\n"
     ]
    }
   ],
   "source": [
    "SPACE = 'SPACE'\n",
    "BLOCK = 'BLOCK'\n",
    "GOAL = 'GOAL'\n",
    "\n",
    "maze_grid = {(0, 0): SPACE, (0, 1): BLOCK, (0, 2): SPACE, (0, 3): SPACE, (0, 4): SPACE, \n",
    "             (0, 5): SPACE, (0, 6): SPACE, (0, 7): SPACE, (1, 0): SPACE, (1, 1): BLOCK,\n",
    "             (1, 2): BLOCK, (1, 3): SPACE, (1, 4): BLOCK, (1, 5): BLOCK, (1, 6): BLOCK, \n",
    "             (1, 7): BLOCK, (2, 0): SPACE, (2, 1): BLOCK, (2, 2): SPACE, (2, 3): SPACE, \n",
    "             (2, 4): SPACE, (2, 5): SPACE, (2, 6): BLOCK, (2, 7): SPACE, (3, 0): SPACE, \n",
    "             (3, 1): SPACE, (3, 2): SPACE, (3, 3): BLOCK, (3, 4): BLOCK, (3, 5): SPACE, \n",
    "             (3, 6): BLOCK, (3, 7): SPACE, (4, 0): SPACE, (4, 1): BLOCK, (4, 2): SPACE, \n",
    "             (4, 3): BLOCK, (4, 4): SPACE, (4, 5): SPACE, (4, 6): SPACE, (4, 7): SPACE, \n",
    "             (5, 0): BLOCK, (5, 1): BLOCK, (5, 2): SPACE, (5, 3): BLOCK, (5, 4): SPACE, \n",
    "             (5, 5): BLOCK, (5, 6): SPACE, (5, 7): BLOCK, (6, 0): SPACE, (6, 1): BLOCK, \n",
    "             (6, 2): BLOCK, (6, 3): BLOCK, (6, 4): SPACE, (6, 5): BLOCK, (6, 6): SPACE, \n",
    "             (6, 7): SPACE, (7, 0): SPACE, (7, 1): SPACE, (7, 2): SPACE, (7, 3): SPACE, \n",
    "             (7, 4): SPACE, (7, 5): BLOCK, (7, 6): BLOCK, (7, 7): GOAL}\n",
    "\n",
    "grid = np.zeros((8, 8)).astype(str)\n",
    "\n",
    "for (i, j), k in maze_grid.items():\n",
    "    if k == SPACE:\n",
    "        grid[i, j] = '0'\n",
    "    elif k == BLOCK:\n",
    "        grid[i, j] = '-'\n",
    "    else:\n",
    "        grid[i, j] = 'x'\n",
    "\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = GridMaze1(grid=maze_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Map:\n",
      "--------------------\n",
      "From State Position(x=0, y=0):\n",
      "  With Action D:\n",
      "    To [State Position(x=1, y=0) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=0, y=2):\n",
      "  With Action R:\n",
      "    To [State Position(x=0, y=3) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=0, y=3):\n",
      "  With Action D:\n",
      "    To [State Position(x=1, y=3) and Reward -1.000] with Probability 1.000\n",
      "  With Action R:\n",
      "    To [State Position(x=0, y=4) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=0, y=2) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=0, y=4):\n",
      "  With Action R:\n",
      "    To [State Position(x=0, y=5) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=0, y=3) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=0, y=5):\n",
      "  With Action R:\n",
      "    To [State Position(x=0, y=6) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=0, y=4) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=0, y=6):\n",
      "  With Action R:\n",
      "    To [State Position(x=0, y=7) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=0, y=5) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=0, y=7):\n",
      "  With Action L:\n",
      "    To [State Position(x=0, y=6) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=1, y=0):\n",
      "  With Action D:\n",
      "    To [State Position(x=2, y=0) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=0, y=0) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=1, y=3):\n",
      "  With Action D:\n",
      "    To [State Position(x=2, y=3) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=0, y=3) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=2, y=0):\n",
      "  With Action D:\n",
      "    To [State Position(x=3, y=0) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=1, y=0) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=2, y=2):\n",
      "  With Action D:\n",
      "    To [State Position(x=3, y=2) and Reward -1.000] with Probability 1.000\n",
      "  With Action R:\n",
      "    To [State Position(x=2, y=3) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=2, y=3):\n",
      "  With Action U:\n",
      "    To [State Position(x=1, y=3) and Reward -1.000] with Probability 1.000\n",
      "  With Action R:\n",
      "    To [State Position(x=2, y=4) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=2, y=2) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=2, y=4):\n",
      "  With Action R:\n",
      "    To [State Position(x=2, y=5) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=2, y=3) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=2, y=5):\n",
      "  With Action D:\n",
      "    To [State Position(x=3, y=5) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=2, y=4) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=2, y=7):\n",
      "  With Action D:\n",
      "    To [State Position(x=3, y=7) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=3, y=0):\n",
      "  With Action D:\n",
      "    To [State Position(x=4, y=0) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=2, y=0) and Reward -1.000] with Probability 1.000\n",
      "  With Action R:\n",
      "    To [State Position(x=3, y=1) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=3, y=1):\n",
      "  With Action R:\n",
      "    To [State Position(x=3, y=2) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=3, y=0) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=3, y=2):\n",
      "  With Action D:\n",
      "    To [State Position(x=4, y=2) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=2, y=2) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=3, y=1) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=3, y=5):\n",
      "  With Action D:\n",
      "    To [State Position(x=4, y=5) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=2, y=5) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=3, y=7):\n",
      "  With Action D:\n",
      "    To [State Position(x=4, y=7) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=2, y=7) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=4, y=0):\n",
      "  With Action U:\n",
      "    To [State Position(x=3, y=0) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=4, y=2):\n",
      "  With Action D:\n",
      "    To [State Position(x=5, y=2) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=3, y=2) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=4, y=4):\n",
      "  With Action D:\n",
      "    To [State Position(x=5, y=4) and Reward -1.000] with Probability 1.000\n",
      "  With Action R:\n",
      "    To [State Position(x=4, y=5) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=4, y=5):\n",
      "  With Action U:\n",
      "    To [State Position(x=3, y=5) and Reward -1.000] with Probability 1.000\n",
      "  With Action R:\n",
      "    To [State Position(x=4, y=6) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=4, y=4) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=4, y=6):\n",
      "  With Action D:\n",
      "    To [State Position(x=5, y=6) and Reward -1.000] with Probability 1.000\n",
      "  With Action R:\n",
      "    To [State Position(x=4, y=7) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=4, y=5) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=4, y=7):\n",
      "  With Action U:\n",
      "    To [State Position(x=3, y=7) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=4, y=6) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=5, y=2):\n",
      "  With Action U:\n",
      "    To [State Position(x=4, y=2) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=5, y=4):\n",
      "  With Action D:\n",
      "    To [State Position(x=6, y=4) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=4, y=4) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=5, y=6):\n",
      "  With Action D:\n",
      "    To [State Position(x=6, y=6) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=4, y=6) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=6, y=0):\n",
      "  With Action D:\n",
      "    To [State Position(x=7, y=0) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=6, y=4):\n",
      "  With Action D:\n",
      "    To [State Position(x=7, y=4) and Reward -1.000] with Probability 1.000\n",
      "  With Action U:\n",
      "    To [State Position(x=5, y=4) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=6, y=6):\n",
      "  With Action U:\n",
      "    To [State Position(x=5, y=6) and Reward -1.000] with Probability 1.000\n",
      "  With Action R:\n",
      "    To [State Position(x=6, y=7) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=6, y=7):\n",
      "  With Action D:\n",
      "    To [Terminal State Position(x=7, y=7) and Reward 0.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=6, y=6) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=7, y=0):\n",
      "  With Action U:\n",
      "    To [State Position(x=6, y=0) and Reward -1.000] with Probability 1.000\n",
      "  With Action R:\n",
      "    To [State Position(x=7, y=1) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=7, y=1):\n",
      "  With Action R:\n",
      "    To [State Position(x=7, y=2) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=7, y=0) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=7, y=2):\n",
      "  With Action R:\n",
      "    To [State Position(x=7, y=3) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=7, y=1) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=7, y=3):\n",
      "  With Action R:\n",
      "    To [State Position(x=7, y=4) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=7, y=2) and Reward -1.000] with Probability 1.000\n",
      "From State Position(x=7, y=4):\n",
      "  With Action U:\n",
      "    To [State Position(x=6, y=4) and Reward -1.000] with Probability 1.000\n",
      "  With Action L:\n",
      "    To [State Position(x=7, y=3) and Reward -1.000] with Probability 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Transition Map:\")\n",
    "print(\"--\" * 10)\n",
    "print(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished value iteration algorithm in 15 iterations\n"
     ]
    }
   ],
   "source": [
    "# frog_mdp : FiniteMarkovDecisionProcess[Position, str] = FrogEscape(length = length)\n",
    "TOLERANCE = 1.e-6\n",
    "iter = 0\n",
    "old_vf : Dict[Position, float] = {s: 0.0 for s in maze.non_terminal_states}\n",
    "vf_generator = value_iteration(mdp = maze, gamma = 1.0)\n",
    "new_vf = next(vf_generator)\n",
    "for new_vf in vf_generator:\n",
    "    if done(old_vf, new_vf, TOLERANCE):\n",
    "        break\n",
    "    old_vf = new_vf\n",
    "    iter += 1\n",
    "\n",
    "print(f\"Finished value iteration algorithm in {iter} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=Position(x=0, y=0)): -15.0, NonTerminal(state=Position(x=0, y=2)): -11.0, NonTerminal(state=Position(x=0, y=3)): -10.0, NonTerminal(state=Position(x=0, y=4)): -11.0, NonTerminal(state=Position(x=0, y=5)): -12.0, NonTerminal(state=Position(x=0, y=6)): -13.0, NonTerminal(state=Position(x=0, y=7)): -14.0, NonTerminal(state=Position(x=1, y=0)): -14.0, NonTerminal(state=Position(x=1, y=3)): -9.0, NonTerminal(state=Position(x=2, y=0)): -13.0, NonTerminal(state=Position(x=2, y=2)): -9.0, NonTerminal(state=Position(x=2, y=3)): -8.0, NonTerminal(state=Position(x=2, y=4)): -7.0, NonTerminal(state=Position(x=2, y=5)): -6.0, NonTerminal(state=Position(x=2, y=7)): -6.0, NonTerminal(state=Position(x=3, y=0)): -12.0, NonTerminal(state=Position(x=3, y=1)): -11.0, NonTerminal(state=Position(x=3, y=2)): -10.0, NonTerminal(state=Position(x=3, y=5)): -5.0, NonTerminal(state=Position(x=3, y=7)): -5.0, NonTerminal(state=Position(x=4, y=0)): -13.0, NonTerminal(state=Position(x=4, y=2)): -11.0, NonTerminal(state=Position(x=4, y=4)): -5.0, NonTerminal(state=Position(x=4, y=5)): -4.0, NonTerminal(state=Position(x=4, y=6)): -3.0, NonTerminal(state=Position(x=4, y=7)): -4.0, NonTerminal(state=Position(x=5, y=2)): -12.0, NonTerminal(state=Position(x=5, y=4)): -6.0, NonTerminal(state=Position(x=5, y=6)): -2.0, NonTerminal(state=Position(x=6, y=0)): -13.0, NonTerminal(state=Position(x=6, y=4)): -7.0, NonTerminal(state=Position(x=6, y=6)): -1.0, NonTerminal(state=Position(x=6, y=7)): 0.0, NonTerminal(state=Position(x=7, y=0)): -12.0, NonTerminal(state=Position(x=7, y=1)): -11.0, NonTerminal(state=Position(x=7, y=2)): -10.0, NonTerminal(state=Position(x=7, y=3)): -9.0, NonTerminal(state=Position(x=7, y=4)): -8.0}\n"
     ]
    }
   ],
   "source": [
    "print(old_vf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the second Reward Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE = 'SPACE'\n",
    "GOAL = 'GOAL'\n",
    "BLOCK = 'BLOCK'\n",
    "\n",
    "DOWN = 'D'\n",
    "UP = 'U'\n",
    "LEFT = 'L'\n",
    "RIGHT = 'R'\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Position:\n",
    "    x : int\n",
    "    y : int\n",
    "\n",
    "ActionMapping = Mapping[Position, Mapping[str, Categorical[Tuple[Position, float]]]]\n",
    "\n",
    "class GridMaze2(FiniteMarkovDecisionProcess[Position, str]):\n",
    "    '''\n",
    "    here, the rewards formulation is that every step gets a reward of 0\n",
    "    the terminal state has a reward of 1\n",
    "    '''\n",
    "    def __init__(self, grid : Dict[Tuple[int, int], str]) -> None:\n",
    "        self.grid = grid\n",
    "        super().__init__(self.GetActionMapping())\n",
    "\n",
    "    def GetActionMapping(self) -> ActionMapping:\n",
    "        d : Dict[Position, Dict[str, Categorical[Tuple[Position, float]]]] = {}\n",
    "\n",
    "        for pos, kind in self.grid.items():\n",
    "            sub_d : Dict[str, Categorical[Tuple[Position, float]]] = {}\n",
    "            if kind == SPACE:\n",
    "                state = Position(x = pos[0], y = pos[1])\n",
    "\n",
    "                down = self.grid.get((pos[0] + 1, pos[1]), BLOCK)\n",
    "                up = self.grid.get((pos[0] - 1, pos[1]), BLOCK)\n",
    "                right = self.grid.get((pos[0], pos[1] + 1), BLOCK)\n",
    "                left = self.grid.get((pos[0], pos[1] - 1), BLOCK)\n",
    "\n",
    "                num_options = (left == SPACE) + (right == SPACE) + (up == SPACE) + (down == SPACE) + (left == GOAL) + (right == GOAL) + (up == GOAL) + (down == GOAL)\n",
    "                p = float(1 / num_options)\n",
    "\n",
    "                if down == SPACE:\n",
    "                    sub_d[DOWN] = Categorical({(Position(x=pos[0] + 1, y=pos[1]), 0.0) : p})\n",
    "\n",
    "                elif down == GOAL:\n",
    "                    sub_d[DOWN] = Categorical({(Position(x=pos[0] + 1, y=pos[1]), 1.0) : p})\n",
    "                \n",
    "                if up == SPACE:\n",
    "                    sub_d[UP] = Categorical({(Position(x=pos[0] - 1, y=pos[1]), 0.0): p})\n",
    "\n",
    "                elif up == GOAL:\n",
    "                    sub_d[UP] = Categorical({(Position(x=pos[0] - 1, y=pos[1]), 1.0): p})\n",
    "                \n",
    "                if right == SPACE:\n",
    "                    sub_d[RIGHT] = Categorical({(Position(x=pos[0], y=pos[1] + 1), 0.0): p})\n",
    "\n",
    "                elif right == GOAL:\n",
    "                    sub_d[RIGHT] = Categorical({(Position(x=pos[0], y=pos[1] + 1), 1.0): p})\n",
    "                \n",
    "                if left == SPACE:\n",
    "                    sub_d[LEFT] = Categorical({(Position(x=pos[0], y=pos[1] - 1), 0.0): p})\n",
    "                elif left == GOAL:\n",
    "                    sub_d[LEFT] = Categorical({(Position(x=pos[0], y=pos[1] - 1), 1.0): p})\n",
    "\n",
    "\n",
    "                d[state] = sub_d\n",
    "\n",
    "        return d\n",
    "\n",
    "def done(v1 : Dict[Position, float], v2 : Dict[Position, float], tol : float):\n",
    "\t'''\n",
    "\tthis function takes in two dictionaries, converts them to numpy\n",
    "\tarrays and then returns True if the maximum absolute value across\n",
    "\tone array and the other is smaller than the specified tolerance\n",
    "\n",
    "\tparameters:\n",
    "\tv1: a dictionary with states as the keys and the elements\n",
    "\t\tin the value function as the values\n",
    "\tv2: a dictionary with states as the keys and the elements\n",
    "\t\tin the value function as the values\n",
    "\ttol: the specified tolerance\n",
    "\n",
    "\treturns:\n",
    "\t--------\n",
    "\tTrue if the maximum difference is less than TOLERANCE\n",
    "\tFalse otherwise\n",
    "\t'''\n",
    "\tarray1 = np.array([i for i in v1.values()])\n",
    "\tarray2 = np.array([i for i in v2.values()])\n",
    "\n",
    "\treturn np.linalg.norm(array1 - array2, ord = np.inf) < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished value iteration algorithm in 16 iterations\n"
     ]
    }
   ],
   "source": [
    "maze = GridMaze2(grid=maze_grid)\n",
    "\n",
    "TOLERANCE = 1.e-6\n",
    "iter = 0\n",
    "\n",
    "old_vf : Dict[Position, float] = {s: 0.0 for s in maze.non_terminal_states}\n",
    "vf_generator = value_iteration(mdp = maze, gamma = 0.9)\n",
    "new_vf = next(vf_generator)\n",
    "\n",
    "for new_vf in vf_generator:\n",
    "    if done(old_vf, new_vf, TOLERANCE):\n",
    "        break\n",
    "    old_vf = new_vf\n",
    "    iter += 1\n",
    "\n",
    "print(f\"Finished value iteration algorithm in {iter} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=Position(x=0, y=0)): 0.2058911320946491, NonTerminal(state=Position(x=0, y=2)): 0.31381059609000017, NonTerminal(state=Position(x=0, y=3)): 0.34867844010000015, NonTerminal(state=Position(x=0, y=4)): 0.31381059609000017, NonTerminal(state=Position(x=0, y=5)): 0.28242953648100017, NonTerminal(state=Position(x=0, y=6)): 0.25418658283290013, NonTerminal(state=Position(x=0, y=7)): 0.22876792454961012, NonTerminal(state=Position(x=1, y=0)): 0.22876792454961012, NonTerminal(state=Position(x=1, y=3)): 0.38742048900000015, NonTerminal(state=Position(x=2, y=0)): 0.25418658283290013, NonTerminal(state=Position(x=2, y=2)): 0.38742048900000015, NonTerminal(state=Position(x=2, y=3)): 0.43046721000000016, NonTerminal(state=Position(x=2, y=4)): 0.47829690000000014, NonTerminal(state=Position(x=2, y=5)): 0.5314410000000002, NonTerminal(state=Position(x=2, y=7)): 0.5314410000000002, NonTerminal(state=Position(x=3, y=0)): 0.28242953648100017, NonTerminal(state=Position(x=3, y=1)): 0.31381059609000017, NonTerminal(state=Position(x=3, y=2)): 0.34867844010000015, NonTerminal(state=Position(x=3, y=5)): 0.5904900000000002, NonTerminal(state=Position(x=3, y=7)): 0.5904900000000002, NonTerminal(state=Position(x=4, y=0)): 0.25418658283290013, NonTerminal(state=Position(x=4, y=2)): 0.31381059609000017, NonTerminal(state=Position(x=4, y=4)): 0.5904900000000002, NonTerminal(state=Position(x=4, y=5)): 0.6561000000000001, NonTerminal(state=Position(x=4, y=6)): 0.7290000000000001, NonTerminal(state=Position(x=4, y=7)): 0.6561000000000001, NonTerminal(state=Position(x=5, y=2)): 0.28242953648100017, NonTerminal(state=Position(x=5, y=4)): 0.5314410000000002, NonTerminal(state=Position(x=5, y=6)): 0.81, NonTerminal(state=Position(x=6, y=0)): 0.25418658283290013, NonTerminal(state=Position(x=6, y=4)): 0.47829690000000014, NonTerminal(state=Position(x=6, y=6)): 0.9, NonTerminal(state=Position(x=6, y=7)): 1.0, NonTerminal(state=Position(x=7, y=0)): 0.28242953648100017, NonTerminal(state=Position(x=7, y=1)): 0.31381059609000017, NonTerminal(state=Position(x=7, y=2)): 0.34867844010000015, NonTerminal(state=Position(x=7, y=3)): 0.38742048900000015, NonTerminal(state=Position(x=7, y=4)): 0.43046721000000016}\n"
     ]
    }
   ],
   "source": [
    "print(old_vf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
